{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7d462a",
   "metadata": {},
   "source": [
    "# Data Cleaning Notebook Template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa8dd7",
   "metadata": {},
   "source": [
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4354a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138f103",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset has no header, we manually assign column names\n",
    "COLUMN_NAMES = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"../data/raw_dataset.csv\", names=COLUMN_NAMES, skipinitialspace=True, na_values='?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e744b",
   "metadata": {},
   "source": [
    "## Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6876fa36",
   "metadata": {},
   "source": [
    "### Dataset Info and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bddfedc",
   "metadata": {},
   "source": [
    "### Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f03214",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9475f8",
   "metadata": {},
   "source": [
    "### Duplicate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae335ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e697f",
   "metadata": {},
   "source": [
    "### Before Cleaning Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81863b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_stats = {\n",
    "    'shape': df.shape,\n",
    "    'total_rows': len(df),\n",
    "    'sample_rows': df.head(15),\n",
    "    'missing_total': missing_values.sum(),\n",
    "    'duplicates': duplicates,\n",
    "}\n",
    "\n",
    "print(\"Before Cleaning Snapshot:\")\n",
    "print(f\"    Shape: {before_stats['shape'][0]:,} rows x {before_stats['shape'][1]} columns\")\n",
    "print(f\"    Sample Rows:\")\n",
    "display(before_stats['sample_rows'])\n",
    "print(f\"    Total Missing Values: {before_stats['missing_total']:,} ({(before_stats['missing_total'] / before_stats['total_rows']) * 100:.2f}%)\")\n",
    "print(f\"    Duplicate Rows: {before_stats['duplicates']:,} ({(before_stats['duplicates'] / before_stats['total_rows']) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf6a41",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1df139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de2d01",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = df_clean.columns[df_clean.isnull().any()].tolist()\n",
    "\n",
    "if cols_with_missing:\n",
    "    print(f\"Columns with missing values: {cols_with_missing}\\n\")\n",
    "    \n",
    "    # Strategy: Mode imputation for categorical columns\n",
    "    for col in cols_with_missing:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            mode_value = df_clean[col].mode()[0]\n",
    "            df_clean.fillna({col: mode_value}, inplace=True)\n",
    "            print(f\"✓ Filled '{col}' with mode: '{mode_value}'\")\n",
    "        else:\n",
    "            median_value = df_clean[col].median()\n",
    "            df_clean.fillna({col: median_value}, inplace=True)\n",
    "            print(f\"✓ Filled '{col}' with median: {median_value}\")\n",
    "else:\n",
    "    print(\"No missing values to handle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5251d0",
   "metadata": {},
   "source": [
    "## Standardize Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from all string columns\n",
    "for col in df_clean.select_dtypes(include=['object']).columns:\n",
    "    df_clean[col] = df_clean[col].str.strip()\n",
    "    print(f\"✓ Stripped whitespace from '{col}'\")\n",
    "\n",
    "# Standardize income labels\n",
    "df_clean['income'] = df_clean['income'].str.replace('.', '', regex=False)\n",
    "print(f\"✓ Standardized 'income' labels (removed periods)\")\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    df_clean[col] = df_clean[col].astype('category')\n",
    "print(f\"✓ Converted {len(categorical_cols)} columns to category dtype\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257bfa08",
   "metadata": {},
   "source": [
    "## Outlier Detection & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detecting and Treating Outliers...\\n\")\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Exclude ordinal/categorical numeric columns\n",
    "exclude_cols = ['education-num']  # Add other columns to exclude if needed\n",
    "numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"Numerical columns identified: {numerical_cols}\\n\")\n",
    "\n",
    "outlier_info = {}\n",
    "for col in numerical_cols:\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_mask = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)\n",
    "    outlier_count = outliers_mask.sum()\n",
    "    outlier_info[col] = outlier_count\n",
    "    \n",
    "    print(f\"'{col}': {outlier_count:,} outliers detected\")\n",
    "    \n",
    "    # Cap outliers using IQR method\n",
    "    df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "print(f\"\\n✓ Outliers capped using IQR method (1.5 × IQR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65a314",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015197c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_duplicates = df_clean.duplicated().sum()\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "df_clean.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97510486",
   "metadata": {},
   "source": [
    "## After Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caabc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_stats = {\n",
    "    'shape': df_clean.shape,\n",
    "    'total_rows': len(df_clean),\n",
    "    'sample_rows': df_clean.head(15),\n",
    "    'missing_total': df_clean.isnull().sum().sum(),\n",
    "    'duplicates': df_clean.duplicated().sum(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee531e8",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3.1 Shapes Comparison\n",
    "\n",
    "print(\"SHAPES COMPARISON\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Before: {before_stats['shape'][0]:,} rows × {before_stats['shape'][1]} columns\")\n",
    "print(f\"After:  {after_stats['shape'][0]:,} rows × {after_stats['shape'][1]} columns\")\n",
    "print(f\"Rows removed: {before_stats['shape'][0] - after_stats['shape'][0]:,}\")\n",
    "\n",
    "# ## 3.2 Sample Rows Comparison\n",
    "\n",
    "print(\"\\n=== BEFORE (First 15 rows) ===\")\n",
    "display(before_stats['sample_rows'])\n",
    "\n",
    "print(\"\\n=== AFTER (First 15 rows) ===\")\n",
    "display(after_stats['sample_rows'])\n",
    "\n",
    "# ## 3.3 Data Quality Metrics\n",
    "\n",
    "print(\"DATA QUALITY METRICS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Missing Values:\")\n",
    "print(f\"  Before: {before_stats['missing_total']:,}\")\n",
    "print(f\"  After:  {after_stats['missing_total']:,}\")\n",
    "print()\n",
    "print(f\"Duplicates:\")\n",
    "print(f\"  Before: {before_stats['duplicates']:,}\")\n",
    "print(f\"  After:  {after_stats['duplicates']:,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf84c1",
   "metadata": {},
   "source": [
    "## Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"../data/cleaned_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3cb59",
   "metadata": {},
   "source": [
    "## Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CLEANING PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✓ Rows: {before_stats['shape'][0]:,} → {after_stats['shape'][0]:,}\")\n",
    "print(f\"✓ Missing values: {before_stats['missing_total']:,} → {after_stats['missing_total']:,}\")\n",
    "print(f\"✓ Duplicates removed: {before_stats['duplicates']:,} → {temp_duplicates:,} → {after_stats['duplicates']:,}\")\n",
    "print(f\"✓ Outliers treated: {sum(outlier_info.values()):,} values capped\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ## Verify Cleaned Dataset\n",
    "\n",
    "# Quick verification\n",
    "print(\"\\nFinal Dataset Info:\")\n",
    "df_clean.info()\n",
    "print(df_clean.describe(include='all'))\n",
    "\n",
    "print(\"\\nCleaning process complete! The dataset is now ready for analysis and modeling.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
